{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Analysis of Mohs Hardness Data Using Bayesian Parameter Optimization\n",
    "\n",
    "The Mohs Hardness scale rates the hardness of different minerals. The data used here combine both real data from mineral data and crystal data as well as a dataset generated using a deep learning model. The process that we have employed to predict the Mohs Hardness value uses different machine learning models: Random Forest, XGBoost, Light GMBoost. To optimize the parameters of each of these models, we used two different optimizers, Bayesian and Optuna, and evaluated our outcomes based on the Median Absolute Error.  \n",
    "\n",
    "The process that we followed included\n",
    "- Data Cleaning\n",
    "- Data Transformation\n",
    "- Creation of Training and Validation Sets\n",
    "- Machine Learning Preprocessing\n",
    "- ML Parameter Optimization\n",
    "- ML Model Validation\n",
    "- ML Model Prediction\n",
    "\n",
    "---\n",
    "## Set up Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "import pprint\n",
    "import joblib\n",
    "from time import time\n",
    "from functools import partial\n",
    "\n",
    "# Sklearn functions\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import median_absolute_error, make_scorer\n",
    "from sklearn.model_selection import KFold, cross_validate, train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Skopt functions for hyperparameter optimization\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.callbacks import DeadlineStopper, DeltaYStopper\n",
    "from skopt.space import Real, Integer\n",
    "\n",
    "# Optuna for hyperparameter optimization\n",
    "import optuna\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=pd.read_csv(\"/kaggle/input/playground-series-s3e25/train.csv\")\n",
    "crystals=pd.read_csv(\"/kaggle/input/prediction-of-mohs-hardness-with-machine-learning/jm79zfps6b-1/Artificial_Crystals_Dataset.csv\")\n",
    "minerals=pd.read_csv(\"/kaggle/input/prediction-of-mohs-hardness-with-machine-learning/jm79zfps6b-1/Mineral_Dataset_Supplementary_Info.csv\")\n",
    "\n",
    "df_test=pd.read_csv(\"/kaggle/input/playground-series-s3e25/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data come from different sources, we need to look at the column naming and dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.columns\n",
    "crystals.columns\n",
    "minerals.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This includes the 'Unnamed: 0', 'Formula', and 'Crystal Structure' columns that are not inlcuded in the training/test dataframes, and is missing the 'id.' It also labels the 'Hardness' as 'Hardness (Mohs)' which will need to be modified to match.\n",
    "\n",
    "This also includes the 'Unnamed: 0'and 'Hardness (Mohs)' which will need to be modified to match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crystals.drop(['Unnamed: 0','Formula', 'Crystal structure'], axis=1, inplace=True)\n",
    "crystals.rename(columns={\"Hardness (Mohs)\": \"Hardness\"}, inplace=True)\n",
    "\n",
    "minerals.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "minerals.rename(columns={\"Hardness (Mohs)\": \"Hardness\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crystals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minerals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training dataset includes the id in numerical order, which matches the index. When we add the crystals and minerals data sets to the training dataframe, it will be necessary to create new 'id' numbers for each of the additions to the set, following the values at the end of the training data to maintain unique id numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create id numbers for the crystals starting after the end of the ID numbers for the original dataset\n",
    "\n",
    "crystals.index = [i + 1 for i in range(len(df_train), len(df_train) + len(crystals))] \n",
    "train_with_crystals=pd.concat([df_train, crystals])\n",
    "\n",
    "# Make the id number the same as the index \n",
    "train_with_crystals.id = train_with_crystals.index\n",
    "\n",
    "# Do the same for the minerals now\n",
    "minerals.index = [i + 1 for i in range(len(train_with_crystals), len(train_with_crystals) + len(minerals))]\n",
    "train_full=pd.concat([train_with_crystals, minerals])\n",
    "train_full.id = train_full.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.columns == train_full.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Data Cleaning\n",
    "\n",
    "The above now verifies that all of the column names are matching with the original data and the new appended dataset. Since we added new data, it's possible that some of the values are duplicated. We had previously looked for duplicates in the training data (not shown), and there were not. It's also important to note that the ID must be removed, since all ID values are unique. By setting the ID as the index, we will no longer be using the ID as a feature, which is also important in the training, while still maintaining the value needed for identification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the ID numbers as the index, then look for duplicates\n",
    "\n",
    "train_full.set_index('id', inplace=True)\n",
    "len(train_full[train_full.duplicated()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found 24 of the newly added rows from the crystals and minerals were duplicates with the training set. We will drop the duplicates using drop_duplicates()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the duplicates \n",
    "train_full.drop_duplicates(inplace=True)\n",
    "len(train_full[train_full.duplicated()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that there are no remaining duplicate rows, there are other issues we need to look for. As what we are looking at in the columns are features such as molecular weight, number of electrons, and density, none of these can have a value of zero. It is possible to have a 0.0 Covelant Bonding, particularly if the mineral is formed with all ionic bonds. A simple example of this is NaCl - non-iodized table salt. But with that one exception, all other features should be greater than zero, and most should be greater than 1. The lowest molecular weight is 1.008 for Hydrogen, which is a gas, not a mineral. So any values for things like total number of electrons, weights, etc. must be greater than 1. \n",
    "\n",
    "At this point, we also have not looked to see if there are nulls present in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count null values\n",
    "train_full.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for problems with minima - all values should be greater than 0, most should be greater than or equal to 1\n",
    "train_full.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lowest Hardness in the Mohs Hardness scale is 1.0, so this feature seems to have an accurate measure. The rest of the columns have problems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elec_tot = (train_full[train_full.allelectrons_Total == 0])\n",
    "den_tot = (train_full[train_full.density_Total == 0])\n",
    "elec_avg = (train_full[train_full.allelectrons_Average == 0])\n",
    "val_e_avg = (train_full[train_full.val_e_Average == 0])\n",
    "weight_avg = (train_full[train_full.atomicweight_Average == 0])\n",
    "ion_avg = (train_full[train_full.ionenergy_Average == 0])\n",
    "chi_avg = (train_full[train_full.el_neg_chi_Average == 0])\n",
    "vdw = (train_full[train_full.R_vdw_element_Average == 0])\n",
    "cov = (train_full[train_full.R_cov_element_Average == 0])\n",
    "zaratio = (train_full[train_full.zaratio_Average == 0])\n",
    "den_avg = (train_full[train_full.density_Average == 0])\n",
    "\n",
    "print([len(elec_tot), len(den_tot), len(elec_avg), len(val_e_avg), len(weight_avg), len(ion_avg), len(chi_avg), len(vdw), len(cov), len(zaratio), len(den_avg)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numbers above show the number of 0.0 values in each of the columns. Since there are likely rows that contain multiple 0.0 valued features, we decided to start with the rows with the highest frequency that are impossible to have a 0.0 value. First, we can look at the atomic weight average. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_avg.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even in the first 10, we can see that 8 of the rows have multiple columns with 0.0 values, which were likely nulls that were replaced with zeros. Since we know the atomic weight must be greater than 1, we can filter for this to see what other features are still all set to 0.0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full.loc[train_full.atomicweight_Average > 1.0].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By isolating the atomic weights above 1 and reexamining the minima, the next physical quality that should always be greater than 1 is the all_electrons total. Again, even a single hydrogen atom has 1 electron, and a mineral MUST have more electrons than hydrogen. The minimum was determined to be 0.001, which is a strange value to say the least."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full.loc[train_full.allelectrons_Total == 0.001]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Somehow the allelectrons_Total yielded a 0.001 value, while the allelectrons_Average is 6.000. THIS MAKES NO SENSE! I am losing faith in the performance of the Deep Learning that was used to create these values, since, based on their IDs, they are not from the added crystal and mineral values. These also each have 4 valence electrons, but VERY different atomic weights and VERY different hardnesses. Anyway, I think we can all agree that the allelectrons_total needs to be greater than 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_train = train_full.loc[(train_full.atomicweight_Average > 1.0) & (train_full.allelectrons_Total > 1.0)]\n",
    "clean_train.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuing with the examination of spurious values, we should now look at the densities. Both the total and average have 0.0 minima. We should look at both. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_train[clean_train.density_Total == 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(clean_train[clean_train.density_Average == 0.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So only one value from the total density yielded a bunch of zeros. The Average density had 40 values. We can just eliminate the one from density total, since much of the data within this row are 0.0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_train = clean_train.loc[clean_train.density_Total > 0.0]\n",
    "clean_train.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...And the suspicious 0.001 returns in density total, so let's look at what's happening there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_train.loc[clean_train.density_Total <= 0.01]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three more... so we can just filter out any under 0.01. Finally, looking at the density Average. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(clean_train[clean_train.density_Average == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_train[clean_train.density_Average == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These 39 rows actually have all data from the other features included. So we're left with two options: \n",
    "- Remove the 39 rows to include the density_Average as a feature for the machine learning. \n",
    "- Keep the 39 rows and remove density_Average as a feature for the machine learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "39/len(clean_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rows represent 0.356% of the full dataset for training, so unless we do decide to remove the feature for density_Average, it seems like a better decision to remove the 39 rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_train = clean_train.loc[clean_train.density_Total > 0.01]\n",
    "clean_train = clean_train.loc[clean_train.density_Average > 0.0]\n",
    "clean_train.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is only 1 remaining feaure with a value equal to zero. It is the covalent bond average, and as mentioned above, there are possible minerals with 0.0 covelant bond energy if all of the bonds in the molecule are ionic. Van der Waals forces would still be present, as these are the forces between non-bonded atomic nuclei. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_train.loc[clean_train.R_cov_element_Average == 0.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since all of the other features have nonzero numerical values, we will keep this row in the training set.  \n",
    "Now that the data is clean, we can look at the distributions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize': (20,20)})\n",
    "clean_train.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Immediately, we can see that the total electrons and total density are heavily skewed right, and likely have outliers. The atomic weight, allelectrons average and density average are also skewed right. \n",
    "\n",
    "## Data Transformation\n",
    "\n",
    "The two element_Averages for Covalent and van der Waals are distances, which are proportional to the cubed root of density. Since the distributions for the element_Averages appear to be closer to a normal distribution, we opted to transform the density as well as the total and average electrons with a cubed root function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_train[['allelectrons_Total_T', 'allelectrons_Average_T', 'density_Total_T']] = clean_train[['allelectrons_Total', 'allelectrons_Average', 'density_Total']].apply(lambda x: x**(1/3))\n",
    "clean_train.drop(['allelectrons_Total', 'allelectrons_Average', 'density_Total'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_train.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify processing the original data, we created the following function to perform all of the steps in the cleaning. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rock_cleaner(df): \n",
    "    df = df.loc[(df.atomicweight_Average > 1.0)] # Removes all atomic weights that cannot possibly exist\n",
    "    df = df.loc[(df.allelectrons_Total > 1.0)] # Removes both non-existent values from the low end\n",
    "    df = df.loc[(df.density_Total > 0.1)] # Removes those values with spurious and low densities\n",
    "    df = df.loc[df.density_Average > 0.0] # Removes the rows with density average nulls set to zero (36 rows)\n",
    "    \n",
    "    df[['allelectrons_Total_T', 'allelectrons_Average_T', 'density_Total_T']] = df[['allelectrons_Total', 'allelectrons_Average', 'density_Total']].apply(lambda x: np.log(x+1))\n",
    "    df.drop(['allelectrons_Total', 'allelectrons_Average', 'density_Total'], axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull the original data and use the cleaning function\n",
    "\n",
    "train_clean = rock_cleaner(train_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of Training and Validation Sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=train_clean.Hardness\n",
    "X=train_clean.drop(['Hardness'], axis=1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame(data=scaler.fit_transform(X), columns=X.columns, index=X.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X_scaled, y,test_size=0.2,\n",
    "                                                                random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Preprocessing\n",
    "\n",
    "We used a Stratified K Fold for the cross validation, which required stratification of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratify the Target\n",
    "y_stratified = pd.cut(y_train.rank(method='first'), bins=20, labels=False)\n",
    "\n",
    "# Set up the scoring function\n",
    "scoring = make_scorer(partial(median_absolute_error), greater_is_better=False)\n",
    "\n",
    "# Set the validation strategy\n",
    "skf = StratifiedKFold(n_splits=7\n",
    "                      , shuffle=True \n",
    "                      , random_state=random_state)\n",
    "\n",
    "cv_strategy = list(skf.split(X_train, y_stratified))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first model we tested was a Random Forest Regression model, which we would tune the hyperparameters using Bayesian Cross Validation or using Optuna. \n",
    "\n",
    "## Random Forest Parameter Optimization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the regressor\n",
    "\n",
    "reg = RandomForestRegressor(\n",
    "    random_state=random_state \n",
    "    )\n",
    "\n",
    "# Setting the search space\n",
    "search_spaces = {'n_estimators': Integer(100, 2000)\n",
    "                 , 'max_depth': Integer(2, 30)\n",
    "                 , 'min_samples_split': Integer(2, 10)\n",
    "                 , 'min_samples_leaf': Integer(1, 10)\n",
    "\n",
    "            }\n",
    "\n",
    "# Wrap everything up into the Bayesian Optimizer\n",
    "opt = BayesSearchCV(estimator=reg\n",
    "                    , search_spaces=search_spaces\n",
    "                    , scoring=scoring\n",
    "                    , cv=cv_strategy\n",
    "                    , n_iter=120                                    # max number of trials\n",
    "                    , n_points=1                                    # number of hyperparameter sets eval at same time\n",
    "                    , n_jobs=1                                      \n",
    "                    , return_train_score=False\n",
    "                    , refit=False\n",
    "                    , optimizer_kwargs={'base_estimator': 'GP'}     # Gaussian optimizeer parameters\n",
    "                    , random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the optimizer \n",
    "overdone_control = DeltaYStopper(delta=0.001)                    # We stop if the gain of the optimization becomes too small\n",
    "time_limit_control = DeadlineStopper(total_time=60)           # Impose Time limit\n",
    "callbacks=[overdone_control, time_limit_control]\n",
    "\n",
    "\n",
    "# opt.fit(X_train, y_train, callback=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best parameters were returned: <br>\n",
    "max_depth: 25<br>\n",
    "min_samples_leaf: 3<br>\n",
    "min_samples_split: 7<br>\n",
    "n_estimators: 1625<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy of the best parameters, so not having to keep running the optimizer\n",
    "reg = RandomForestRegressor(n_estimators=1625\n",
    "    , min_samples_split=7\n",
    "    , min_samples_leaf=3\n",
    "    , max_depth=25\n",
    "    , random_state=random_state)\n",
    "reg.fit(X_train, y_train) \n",
    "rf_predictions_final = reg.predict(X_valid)\n",
    "\n",
    "print(\"Generated Test Data Predictions using Final Model:\\n\", \n",
    "                                                  rf_predictions_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to look at the feature importances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = reg.feature_importances_\n",
    "\n",
    "feature_importance = pd.DataFrame(data=importance, index=X_train.columns, columns=['importance']).sort_values(ascending=True, by='importance')\n",
    "\n",
    "feature_importance.plot(kind='barh', figsize=(12, 8), color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame({'RF Prediction': rf_predictions_final \n",
    "                       , 'Actual Hardness': y_valid})\n",
    "output.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the output needs to be assessed using a median_absolute_error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Med_A_e = median_absolute_error(rf_predictions_final, y_valid)\n",
    "Med_A_e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The other hyperparameter tuning method we used utlitize the Optuna package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, x_train, y_train, cv, scoring):\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 220, 1000, step=20)\n",
    "        , \"max_depth\": trial.suggest_int(\"max_depth\", 4, 19)\n",
    "        , \"random_state\": 42\n",
    "        , \"min_samples_split\" : trial.suggest_int(\"min_samples_split\", 2, 19)\n",
    "        , \"min_samples_leaf\" : trial.suggest_int(\"min_samples_leaf\", 2, 19)\n",
    "    }\n",
    "\n",
    "    gr_reg = RandomForestRegressor(**params)\n",
    "    scores = cross_validate(gr_reg, X_train, y_train, cv=cv, scoring=scoring, n_jobs=-1)\n",
    "    gr_reg.fit(X_train, y_train) \n",
    "\n",
    "    y_pred = gr_reg.predict(X_valid)\n",
    "    \n",
    "    # Compute RMSLE\n",
    "    MedAE = median_absolute_error(y_valid,y_pred)\n",
    "\n",
    "    return MedAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # Create study that minimizes\n",
    "# study = optuna.create_study(direction=\"minimize\")\n",
    "\n",
    "# # Wrap the objective inside a lambda with the relevant arguments\n",
    "# kf = KFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "\n",
    "# # Pass additional arguments inside another function\n",
    "# func = lambda trial: objective(trial, X_train, y_train, cv=kf, scoring=\"neg_mean_squared_log_error\")\n",
    "\n",
    "# # Start optimizing with 100 trials\n",
    "# study.optimize(func, n_trials=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimators using Optuna yielded a similar outcome with different estimators: \n",
    "\n",
    "max_depth: 19<br>\n",
    "min_samples_leaf: 2<br>\n",
    "min_samples_split: 5<br>\n",
    "n_estimators: 920<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_forest_model = RandomForestRegressor(n_estimators=920,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    max_depth=19,\n",
    "    random_state=random_state)\n",
    "basic_forest_model.fit(X_train, y_train) \n",
    "rf_predictions_final = basic_forest_model.predict(X_valid)\n",
    "\n",
    "print(\"Generated Test Data Predictions using Final Model:\\n\", \n",
    "                                                  rf_predictions_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame({'RF Prediction': rf_predictions_final \n",
    "                       , 'Actual Hardness': y_valid})\n",
    "output.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Med_A_e = median_absolute_error(rf_predictions_final, y_valid)\n",
    "Med_A_e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While there wasn't a huge improvement, with the use of the Random Forest Regressor, the tuning with Optuna did yield better hyperparameters. The Bayesian predictors didn't run as many trials as the Optuna, which could have impacted this; however, the callback for the change in y value was set to 0.001. \n",
    "\n",
    "---\n",
    "\n",
    "Additional models we tested: \n",
    "- XGBoost Regression\n",
    "- Light GBM Regressor\n",
    "\n",
    "With each of these models, we used the same data inputs, yet the results that we found were similar but did not perform as well as the Random Forest Model using either the Bayesian or Optuna Tuning. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
